# Feature Specification: RAG Backend API & Chat Integration

**Feature Branch**: `001-rag-backend-chat`
**Created**: 2025-12-07
**Status**: Draft
**Input**: User description: "Feature: RAG Backend API & Chat Integration

I want to implement a complete RAG (Retrieval-Augmented Generation) backend to answer questions based on the textbook content.

**Technical Stack:**
* **Framework**: FastAPI (Python)
* **Deployment**: Vercel Serverless Functions
* **Vector Database**: Qdrant
* **LLM Provider**: Google Gemini (via `google-generativeai` and OpenAI SDK adapter)

**Backend Requirements:**
1.  **Entry Point**: Create `api/index.py` serving as the main FastAPI app.
2.  **Endpoints**:
    * `POST /api/chat`: Accepts a query, searches Qdrant for context, and returns an LLM-generated answer.
    * `POST /api/ask-selection`: Accepts a user-selected text snippet and a question, returning a context-aware explanation.
    * `GET /api/health`: Simple health check.
3.  **Utilities**:
    * `utils/config.py`: Initialize Qdrant client.
    * `utils/models.py`: Pydantic models for `ChatRequest`, `ChatResponse`, `AskSelectionRequest`.
    * `utils/tools.py`: Helper functions for `search_book_content` and `format_context`.
    * `utils/helpers.py`: Logic for `embed_text` using Gemini.
4.  **Configuration**: Use `dotenv` for local development and standard OS env vars for production.
5.  **Dependencies**: Create a `requirements.txt` with `fastapi`, `uvicorn`, `qdrant-client`, `google-genai`, `pydantic`.

**Frontend Integration Requirements:**
1.  **Chat UI**: Port the `ChatBot.tsx` component to `src/components/ChatBot.tsx`.
2.  **API Client**: Create `src/lib/chatApi.ts` to handle fetch requests to the backend.
3.  **Vercel Config**: Create `vercel.json` to route `/api/*` requests to the Python app.

**Success Criteria:**
* The `/api/health` endpoint returns `{"status": "ok"}` locally and on Vercel.
* Sending a question to `/api/chat` returns a valid answer using textbook context."

## User Scenarios & Testing *(mandatory)*

### User Story 1 - Ask a Question (Priority: P1)

As a user, I want to ask a question related to the textbook content and receive an accurate answer generated by the RAG backend, so that I can quickly get information without manually searching the entire textbook.

**Why this priority**: This is the core functionality of the RAG backend.

**Independent Test**: Send a query to the `/api/chat` endpoint and verify that a relevant and accurate answer is returned, referencing information from the textbook content.

**Acceptance Scenarios**:

1.  **Given** the RAG backend is running, **When** I send a POST request to `/api/chat` with a question about the textbook content, **Then** I receive a JSON response containing a generated answer that is relevant and factually accurate based on the textbook content.
2.  **Given** the RAG backend is running, **When** I send a POST request to `/api/chat` with a question unrelated to the textbook content, **Then** I receive a JSON response indicating that the answer cannot be found in the provided context or a general LLM response.

### User Story 2 - Get Context-Aware Explanation (Priority: P1)

As a user, I want to select a text snippet from the textbook and ask a specific question about it, receiving a context-aware explanation, so that I can deepen my understanding of particular sections.

**Why this priority**: This provides a more targeted interaction with the RAG system.

**Independent Test**: Send a user-selected text snippet and a question to the `/api/ask-selection` endpoint and verify that a context-aware explanation is returned.

**Acceptance Scenarios**:

1.  **Given** the RAG backend is running, **When** I send a POST request to `/api/ask-selection` with a text snippet and a question, **Then** I receive a JSON response containing a generated explanation that is relevant to the provided snippet and question.

### User Story 3 - Health Check (Priority: P3)

As a developer, I want to check the health status of the RAG backend, so that I can ensure the API is operational.

**Why this priority**: Essential for monitoring and deployment verification.

**Independent Test**: Send a GET request to the `/api/health` endpoint and verify the status.

**Acceptance Scenarios**:

1.  **Given** the RAG backend is deployed, **When** I send a GET request to `/api/health`, **Then** I receive a JSON response `{"status": "ok"}`.

### Edge Cases

-   What happens if the Qdrant database is unavailable?
-   What happens if the Google Gemini API is unresponsive or returns an error?
-   What happens if an invalid query is sent to `/api/chat` or `/api/ask-selection`?
-   How are rate limits handled for the LLM provider?

## Requirements *(mandatory)*

### Functional Requirements

-   **FR-001**: The backend MUST expose a POST endpoint at `/api/chat` that accepts a user query and returns an LLM-generated answer.
-   **FR-002**: The backend MUST expose a POST endpoint at `/api/ask-selection` that accepts a selected text snippet and a question, returning a context-aware explanation.
-   **FR-003**: The backend MUST expose a GET endpoint at `/api/health` that returns a status of `{"status": "ok"}`.
-   **FR-004**: The backend SHOULD retrieve relevant context from an external, persistent Qdrant instance based on the user's query for the `/api/chat` endpoint. If Qdrant is unavailable, a generic LLM response may be returned.
-   **FR-005**: The backend SHOULD retrieve relevant context from an external, persistent Qdrant instance based on the user's selected text for the `/api/ask-selection` endpoint. If Qdrant is unavailable, a generic LLM response may be returned.
-   **FR-006**: The backend MUST primarily use Google Gemini via `google-generativeai` for LLM responses. If the Gemini API is unresponsive, a predefined generic message or a fallback LLM response (if configured) SHOULD be returned.
-   **FR-007**: The backend MUST use Pydantic models for all API request and response payloads.
-   **FR-008**: The backend MUST structure its main application entry point as `api/index.py` for Vercel Serverless compatibility.
-   **FR-009**: The backend MUST separate shared logic into a `utils/` directory, including `config.py`, `models.py`, `tools.py`, and `helpers.py`.
-   **FR-010**: The backend MUST load sensitive API keys and configuration from environment variables.
-   **FR-011**: The frontend MUST communicate with the backend solely via `/api` endpoints, defined in `src/lib/chatApi.ts`.
-   **FR-012**: The frontend MUST integrate the `ChatBot.tsx` component into `src/components/ChatBot.tsx`.
-   **FR-013**: The project MUST include a `vercel.json` file to route `/api/*` requests to the Python backend.
-   **FR-014**: The backend MUST attempt to parse and cleanse invalid input from `/api/chat` and `/api/ask-selection` requests before processing, to improve robustness against malformed client requests.
-   **FR-015**: The backend MUST return a generic message to the user if rate limits are encountered from the LLM provider, to avoid exposing API errors directly.

### Key Entities *(include if feature involves data)*

-   **ChatRequest**: Pydantic model for `/api/chat` input (e.g., `query: str`).
-   **ChatResponse**: Pydantic model for `/api/chat` output (e.g., `answer: str`, `context: List[str]`).
-   **AskSelectionRequest**: Pydantic model for `/api/ask-selection` input (e.g., `selection: str`, `question: str`).
-   **TextChunk**: Represents a chunk of text from the textbook, possibly with metadata (e.g., `text: str`, `source: str`, `page: int`).

## Success Criteria *(mandatory)*

### Measurable Outcomes

-   **SC-001**: The `/api/health` endpoint returns `{"status": "ok"}` locally and on Vercel within 1 second.
-   **SC-002**: Sending a question to `/api/chat` returns a valid LLM-generated answer within 5 seconds for 95% of requests, using textbook context.
-   **SC-003**: Sending a user-selected text snippet and question to `/api/ask-selection` returns a context-aware explanation within 7 seconds for 95% of requests.
-   **SC-004**: Frontend chat UI successfully sends queries to the backend and displays responses.

## Clarifications

### Session 2025-12-07

- Q: What should be the system's behavior if the Qdrant database is unavailable during a `/api/chat` or `/api/ask-selection` request? → A: Return a specific error message to the user, but allow the LLM to generate a generic response (without RAG).
- Q: What should be the system's behavior if the Google Gemini API is unresponsive or returns an error? → A: Return a specific error message to the user, but use a fallback LLM (if configured) or a predefined generic message.
- Q: How should the system respond if an invalid query (e.g., malformed JSON, missing required fields) is sent to `/api/chat` or `/api/ask-selection`? → A: Attempt to parse/cleanse the invalid input before processing.
- Q: How should the system handle potential rate limits imposed by the LLM provider (Google Gemini)? → A: Return a generic message to the user.
- Q: Will Qdrant persist data across Vercel serverless function invocations, or will it need to be reinitialized/repopulated for each request? → A: Assume a persistent Qdrant instance (e.g., Qdrant Cloud Free Tier, self-hosted). Backend functions connect to this external instance.